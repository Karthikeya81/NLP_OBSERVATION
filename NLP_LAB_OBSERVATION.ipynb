{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNrNfppJoL2762ZR/DzlJ8n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Karthikeya81/NLP_OBSERVATION/blob/main/NLP_LAB_OBSERVATION.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "becX9YsRvvH4"
      },
      "source": [
        "EXP_1 Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXwjA2Qru90S",
        "outputId": "220245b8-4c37-492e-b149-8df32661e972"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences: ['hi how are you;']\n",
            "Words: ['hi', 'how', 'are', 'you', ';']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "\n",
        "text = \"hi how are you;\"\n",
        "\n",
        "\n",
        "sentences = sent_tokenize(text)\n",
        "\n",
        "\n",
        "print(\"Sentences:\", sentences)\n",
        "\n",
        "\n",
        "words = word_tokenize(text)\n",
        "\n",
        "print(\"Words:\", words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c72CSodOv3Vd"
      },
      "source": [
        "EXP_2 Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2UJm0kZwD1r",
        "outputId": "47cb397b-3805-4dfe-bac3-cadeb76b83ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cat', 'running', 'faster', 'than', 'dog', 'after', 'their', 'food']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "# Import the correct class and function\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True) # Added this line to download the missing resource\n",
        "\n",
        "text = \"cats running faster than dogs after their food\"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize the token\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "print(lemmatized_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOLPs87fyVcc"
      },
      "source": [
        "EXP-3 Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wKjyrAtykr7",
        "outputId": "a87e7c81-69a5-4285-b90d-b1a5f36720b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter words separated by space: cats are running very quick than rats\n",
            "\n",
            "--- Stemming Results ---\n",
            "cats -> cat\n",
            "are -> are\n",
            "running -> run\n",
            "very -> veri\n",
            "quick -> quick\n",
            "than -> than\n",
            "rats -> rat\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "\n",
        "words_input = input(\"Enter words separated by space: \")\n",
        "words = words_input.split()\n",
        "\n",
        "print(\"\\n--- Stemming Results ---\")\n",
        "\n",
        "for w in words:\n",
        "    print(f\"{w} -> {stemmer.stem(w)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqXrhokA0Ek0"
      },
      "source": [
        "EXP_4 Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQpqUZ7e0Nb8",
        "outputId": "70edc9e0-9ad6-4a18-8433-ed16097a3207"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " i love nlp\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "\n",
        "text = \" I love NLP!!\"\n",
        "normalized_text = text.lower()\n",
        "normalized_text = normalized_text.translate(\n",
        "    str.maketrans(\"\", \"\", string.punctuation))\n",
        "print(normalized_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8yUhRR-1F_t"
      },
      "source": [
        "EXP_5 Morphology"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmblqZN21M1B",
        "outputId": "b06fab2b-b4fe-4b0a-d96b-3988556d1a59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original tokens: ['running', 'runners', 'easily', 'code']\n",
            "Stemmed words:   ['run', 'runner', 'easili', 'code']\n",
            "Lemmatized words: ['running', 'runner', 'easily', 'code']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "text = \"running runners easily code\"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stems = [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "print(f\"Original tokens: {tokens}\")\n",
        "print(f\"Stemmed words:   {stems}\")\n",
        "print(f\"Lemmatized words: {lemmas}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBUqzaJN29Wc"
      },
      "source": [
        "EXP_6 Spelling Correction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBnPls1a3FPk",
        "outputId": "1d84a584-38e0-46c8-f2da-1f353931dc0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enter a sentence: Honsty is the best polcy \n",
            "Original:  Honsty is the best polcy \n",
            "Corrected:  Honesty is the best policy \n"
          ]
        }
      ],
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "text = input(\"enter a sentence: \")\n",
        "blob = TextBlob(text)\n",
        "corrected = blob.correct()\n",
        "\n",
        "print(\"Original: \", text)\n",
        "print(\"Corrected: \", corrected)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78Uh8rup3buX"
      },
      "source": [
        "EXP_7 Detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_r0Wb0vs9oUe",
        "outputId": "39c98a4b-fd14-40d3-dc5f-ca7ad4d5082d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Premises: \n",
            "  all x.(Man(x) -> Mortal(x))\n",
            "  Man(Socrates)\n",
            "Hypothesis: Mortal(Socrates)\n",
            "Is the hypothesis entailed by the premises? Yes\n",
            "\n",
            "------------------------------\n",
            "\n",
            "Premises: \n",
            "  all x.(Man(x) -> Mortal(x))\n",
            "  Man(Socrates)\n",
            "  -Mortal(Socrates)\n",
            "Do these premises contain a contradiction? Yes\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.sem import Expression\n",
        "from nltk.inference import ResolutionProver\n",
        "\n",
        "\n",
        "p1 = Expression.fromstring('all x (Man(x) -> Mortal(x))')\n",
        "\n",
        "\n",
        "p2 = Expression.fromstring('Man(Socrates)')\n",
        "\n",
        "\n",
        "h = Expression.fromstring('Mortal(Socrates)')\n",
        "\n",
        "\n",
        "premises = [p1, p2]\n",
        "\n",
        "\n",
        "result = ResolutionProver().prove(h, premises, verbose=False)\n",
        "\n",
        "print(f\"Premises: \\n  {p1}\\n  {p2}\")\n",
        "print(f\"Hypothesis: {h}\")\n",
        "print(f\"Is the hypothesis entailed by the premises? {'Yes' if result else 'No'}\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*30 + \"\\n\")\n",
        "\n",
        "p3 = Expression.fromstring('-Mortal(Socrates)')\n",
        "\n",
        "\n",
        "contradiction = Expression.fromstring('Mortal(Socrates) & -Mortal(Socrates)')\n",
        "\n",
        "premises_2 = [p1, p2, p3]\n",
        "result_2 = ResolutionProver().prove(contradiction, premises_2, verbose=False)\n",
        "\n",
        "print(f\"Premises: \\n  {p1}\\n  {p2}\\n  {p3}\")\n",
        "print(f\"Do these premises contain a contradiction? {'Yes' if result_2 else 'No'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaHlpqRE-KuP"
      },
      "source": [
        "EXP-8 N-Grams Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pv1dfYz1A4ay",
        "outputId": "ed25a16b-fbbc-43aa-f9ac-785513da34b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigrams:\n",
            "[('this',), ('is',), ('a',), ('simple',), ('example',), ('for',), ('n',), ('gram',), ('generation',)]\n",
            "Bigrams:\n",
            "[('this', 'is'), ('is', 'a'), ('a', 'simple'), ('simple', 'example'), ('example', 'for'), ('for', 'n'), ('n', 'gram'), ('gram', 'generation')]\n",
            "Trigrams:\n",
            "[('this', 'is', 'a'), ('is', 'a', 'simple'), ('a', 'simple', 'example'), ('simple', 'example', 'for'), ('example', 'for', 'n'), ('for', 'n', 'gram'), ('n', 'gram', 'generation')]\n"
          ]
        }
      ],
      "source": [
        "def generate_ngrams(text, n):\n",
        "    tokens = text.split()\n",
        "    ngrams = []\n",
        "    for i in range(len(tokens) - n + 1):\n",
        "        ngram = tuple(tokens[i:i+n])\n",
        "        ngrams.append(ngram)\n",
        "    return ngrams\n",
        "\n",
        "def generate_all_ngrams(text):\n",
        "    unigrams = generate_ngrams(text, 1)\n",
        "    bigrams = generate_ngrams(text, 2)\n",
        "    trigrams = generate_ngrams(text, 3)\n",
        "    return unigrams, bigrams, trigrams\n",
        "\n",
        "text = \"this is a simple example for n gram generation\"\n",
        "uni, bi, tri = generate_all_ngrams(text)\n",
        "\n",
        "print(\"Unigrams:\")\n",
        "print(uni)\n",
        "print(\"Bigrams:\")\n",
        "print(bi)\n",
        "print(\"Trigrams:\")\n",
        "print(tri)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vd2cRMkHBdQ-"
      },
      "source": [
        "EXP_9 N-Gram Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIxIg3xkBcvI",
        "outputId": "5ed84283-2c66-428f-c0f3-e2ad2d09667b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "p(2|('i', 'love')) = 0.200\n",
            "p(1|('love', 'nlp')) = 0.200\n",
            "p(1|('nlp', '&')) = 0.200\n",
            "p(1|('&', 'i')) = 0.200\n",
            "p(1|('love', 'python')) = 0.200\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "text = \"I love NLP & I love python\"\n",
        "words = text.lower().split()\n",
        "\n",
        "bigrams = [(words[i], words[i+1]) for i in range(len(words)-1)]\n",
        "\n",
        "uni_count = Counter(words)\n",
        "bi_count = Counter(bigrams)\n",
        "\n",
        "v = len(uni_count)\n",
        "\n",
        "for (w1, w2) in bi_count.items():\n",
        "    prob = (bi_count[(w1, w2)] + 1) / (uni_count[w1] + v)\n",
        "    print(f\"p({w2}|{w1}) = {prob:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMMDdI-cBsuQ"
      },
      "source": [
        "EXP_10 POS TAGGING : Hidden Markov"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLjj0-buC9jp",
        "outputId": "d59ae8ee-4670-47fc-b3f7-4885ddb09903"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summary: python is language is used ai datascience development\n"
          ]
        }
      ],
      "source": [
        "nouns = (\"python\", \"language\", \"ai\", \"data\", \"science\", \"development\", \"datascience\")\n",
        "verbs = [\"is\", \"used\", \"study\", \"play\", \"run\", \"love\"]\n",
        "\n",
        "text = \"python is a programming language. it is used for AI, datascience, and web development.\"\n",
        "words = [w.strip(\".,\") for w in text.lower().split()]\n",
        "\n",
        "summary = [w for w in words if w in nouns or w in verbs]\n",
        "\n",
        "print(\"Summary:\", \" \".join(summary))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXP_11 Blending POS TAGGING"
      ],
      "metadata": {
        "id": "rEcnDMicl9D_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import treebank\n",
        "from nltk.tag import RegexpTagger, UnigramTagger\n",
        "\n",
        "# Download corpus\n",
        "nltk.download('treebank')\n",
        "\n",
        "# Load tagged data for training\n",
        "train_sents = treebank.tagged_sents()[:3000]\n",
        "\n",
        "# Step 1: Rule-based tagger (simple regex rules)\n",
        "patterns = [\n",
        "    (r'.*ing$', 'VBG'),   # gerunds\n",
        "    (r'.*ed$', 'VBD'),    # past tense verbs\n",
        "    (r'.*es$', 'VBZ'),    # 3rd person singular present verbs\n",
        "    (r'.*ly$', 'RB'),     # adverbs\n",
        "    (r'.*ion$', 'NN'),    # nouns ending with 'ion'\n",
        "    (r'^[A-Z].*', 'NNP'), # proper nouns\n",
        "    (r'.*', 'NN')         # default: noun\n",
        "]\n",
        "\n",
        "rule_tagger = RegexpTagger(patterns)\n",
        "\n",
        "# Step 2: Statistical tagger (trained on Treebank)\n",
        "unigram_tagger = UnigramTagger(train_sents, backoff=rule_tagger)\n",
        "\n",
        "# Test sentence\n",
        "sentence = \"The quick brown fox is jumping happily\".split()\n",
        "\n",
        "# Tag sentence\n",
        "tags = unigram_tagger.tag(sentence)\n",
        "\n",
        "print(\"Blended POS Tags:\")\n",
        "print(tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnWiPjdklvfz",
        "outputId": "c54390df-f076-4964-fc3e-b1d31587796e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/treebank.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blended POS Tags:\n",
            "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('is', 'VBZ'), ('jumping', 'VBG'), ('happily', 'RB')]\n"
          ]
        }
      ]
    }
  ]
}